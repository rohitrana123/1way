# The outcome of a fair coin is the most uncertain:

import numpy as np
from scipy.stats import entropy

base = 2  # work in units of bits
pk = np.array([1/2, 1/2])  # fair coin
H = entropy(pk, base=base)

# Check equivalence with the formula
H == -np.sum(pk * np.log(pk)) / np.log(base)

# The outcome of a biased coin is less uncertain:
qk = np.array([9/10, 1/10])  # biased coin
H = entropy(qk, base=base)

# Expected output: Entropy of fair coin ≈ 1.0 bit (maximum uncertainty)
# Entropy of biased coin ≈ 0.469 bits (less uncertainty)

# The relative entropy (KL Divergence) between fair and biased coins:
D = entropy(pk, qk, base=base)

# Expected output: D ≈ 0.531 bits — measures difference between pk and qk

# Check equivalence with manual formula
D == np.sum(pk * np.log(pk/qk)) / np.log(base)

# Cross Entropy = Entropy + KL Divergence
CE = entropy(pk, base=base) + entropy(pk, qk, base=base)

# Expected output: CE ≈ 1.531 bits — combined uncertainty

CE == -np.sum(pk * np.log(qk)) / np.log(base)


# ---------------------------------------------------------------------------------
# Entropy of a given probability distribution [0.2, 0.3, 0.5]

import numpy as np

def entropy(prob_dist):
    return -np.sum(prob_dist * np.log2(prob_dist))

# Example
prob_dist = np.array([0.2, 0.3, 0.5])
print("Entropy:", entropy(prob_dist))

# Expected output:
# Entropy: 1.4854752972273344 bits
# Meaning: Moderate uncertainty in the distribution


# ----------------------------------------------------
from sklearn import datasets
from scipy.stats import entropy
import numpy as np

iris = datasets.load_iris()
X = iris.data

# Binary classification: Setosa (0) vs Non-Setosa (1)
y = (iris.target != 0).astype(int)

y_entropy = entropy(np.bincount(y), base=2)

print("Entropy of Iris dataset (binary classification):", y_entropy)
# Expected output:
# Entropy of Iris dataset (binary classification): 0.9182958340544896 bits
# Meaning: The target variable is moderately balanced (some uncertainty)


# -------------------------------------------------
from sklearn import datasets
from scipy.stats import entropy
import numpy as np

wine = datasets.load_wine()
X = wine.data
y = wine.target
y_entropy = entropy(np.bincount(y), base=2)
print("Entropy of Wine dataset (multiclass classification):", y_entropy)

# Expected output:
# Entropy of Wine dataset (multiclass classification): 1.530 bits (approx)
# Meaning: High diversity among the three wine classes


# -------------------------------------------------
# Mutual Information for Feature Selection

from sklearn.feature_selection import mutual_info_classif
from sklearn.datasets import load_iris

# Load dataset
data = load_iris()
X, y = data.data, data.target

# Calculate mutual information
mi = mutual_info_classif(X, y)
print("Mutual Information:", mi)

# Expected output (approximate values may vary slightly):
# Mutual Information: [0.67 0.26 0.78 0.79]
# Meaning: Features 3 & 4 carry the most information about the target variable.


# ---------------------------------------------------------------------------------
# KL Divergence in Python

from scipy.stats import entropy

def kl_divergence(p, q):
    return entropy(p, q)

# Example distributions
p = np.array([0.1, 0.4, 0.5])
q = np.array([0.2, 0.3, 0.5])
print("KL Divergence:", kl_divergence(p, q))

# Expected output:
# KL Divergence: 0.04575811092471789
# Meaning: The two distributions are similar (low divergence)
